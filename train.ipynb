{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4740cba6-5c5f-433d-bd43-c9e561157a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaox/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/zhaox/ProcessGPT/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6b055b-502f-4e54-84cc-6e0c7aae41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/zhaox/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GPTNeoConfig, \n",
    "    GPTNeoForCausalLM\n",
    ")\n",
    "\n",
    "from model import GPT\n",
    "from utils import *  \n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e6f9a0-8201-4eae-a741-a97902a72990",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3407\n",
    "epochs = 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cfg_param = \"33M\"\n",
    "cfg = load_config(f\"configs/config-{cfg_param}.json\")\n",
    "batch_size = 8\n",
    "window_size = cfg[\"window_size\"]\n",
    "lr = cfg[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f55471-c9eb-439a-a90b-9bb3f8da1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "\n",
    "current_time = datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "log_filename = f\"logs/training_{cfg_param}_{current_time}.log\"\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639fbbc9-1945-4187-b879-299e5b6fecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaox/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load dataset and tokenizer\n",
    "\n",
    "model_name = 'roneneldan/TinyStories'\n",
    "dataset = load_dataset('2Xm7/25PofTinyStories')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9d8fc3-4704-421b-91e9-d7c79ac8b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataloaders\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset['validation'], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29079628-8fc8-4e8f-b585-78b612fc9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "\n",
    "config = GPTNeoConfig(\n",
    "    vocab_size=cfg.get('vocab_size', tokenizer.vocab_size),\n",
    "    max_position_embeddings=cfg.get('max_position_embeddings', 2048),\n",
    "    hidden_size=cfg.get('hidden_size', 1024),\n",
    "    num_layers=cfg.get('num_layers', 8),\n",
    "    num_heads=cfg.get('num_heads', 16),\n",
    "    activation_function=cfg.get('activation_function', 'gelu_new'),\n",
    "    attention_types=cfg.get('attention_types', [['global', 'local'], 4]),\n",
    "    attention_layers=cfg.get('attention_layers', ['global', 'local'] * 4),\n",
    "    bos_token_id=cfg.get('bos_token_id', tokenizer.bos_token_id),\n",
    "    eos_token_id=cfg.get('eos_token_id', tokenizer.eos_token_id),\n",
    "    layer_norm_epsilon=cfg.get('layer_norm_epsilon', 1e-5),\n",
    "    initializer_range=cfg.get('initializer_range', 0.02),\n",
    "    use_cache=cfg.get('use_cache', True),\n",
    "    attention_dropout=cfg.get('attention_dropout', 0.0),\n",
    "    resid_dropout=cfg.get('resid_dropout', 0.0),\n",
    "    embed_dropout=cfg.get('embed_dropout', 0.0),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "model = GPTNeoForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d911b5-5eb6-4a4a-9be7-cee62f7b8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust tokenizer and embeddings \n",
    "\n",
    "if tokenizer.vocab_size != config.vocab_size:\n",
    "    tokenizer.add_tokens(['<|endoftext|>'])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg[\"learning_rate\"],\n",
    "    betas=(cfg[\"adam_beta1\"], cfg[\"adam_beta2\"]),\n",
    "    weight_decay=cfg[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "gradient_accumulation_steps = cfg[\"gradient_accumulation_steps\"]\n",
    "\n",
    "updates, start_epoch, start_step = 0, 0, 0\n",
    "model_dir = f\"models/model_{cfg_param}_{current_time}\"\n",
    "resume_training = False\n",
    "if resume_training:\n",
    "    logging.info(f\"Resuming training from {model_dir}\")\n",
    "    model, tokenizer, updates, start_epoch, start_step = load_checkpoint(model_dir, optimizer=optim)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d86e3f2-1d4f-4c83-a7ef-3d8179de5ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxm7zhao\u001b[0m (\u001b[33mxm7zhao-lafayette-college\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zhaox/ProcessGPT/wandb/run-20241029_161855-fknam3j5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P/runs/fknam3j5' target=\"_blank\">gpt-tinystories-25P-28M-1029_161840</a></strong> to <a href='https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P' target=\"_blank\">https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P/runs/fknam3j5' target=\"_blank\">https://wandb.ai/xm7zhao-lafayette-college/gpt-tinystories-25P/runs/fknam3j5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup Weights & Biases\n",
    "run = wandb.init(\n",
    "    project=\"gpt-tinystories-25P\",\n",
    "    name=f\"gpt-tinystories-25P-{cfg_param}-{current_time}\",\n",
    "    config={\n",
    "        \"cfg_param\": cfg_param,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"model_dir\": model_dir,\n",
    "        \"log_filename\": log_filename,\n",
    "        \"seed\": seed,\n",
    "        \"epochs\": epochs\n",
    "    },\n",
    ")\n",
    "logging.info(f\"cfg_param: {cfg_param}, lr: {lr}, batch_size: {batch_size}, \"\n",
    "             f\"model_dir: {model_dir}, log_filename: {log_filename}, \"\n",
    "             f\"seed: {seed}, epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8e77b-9d7c-489d-8871-9600968d67f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                                       | 90/62159 [00:55<10:55:56,  1.58it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    logging.info(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "\n",
    "    if epoch > start_epoch:\n",
    "        start_step = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"), start=start_step):\n",
    "        optim.zero_grad()\n",
    "        inputs = tokenizer(batch['text'], padding=True, return_tensors='pt',\n",
    "                           max_length=window_size, truncation=True)['input_ids'].to(device)\n",
    "        outputs = model(input_ids=inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        updates += 1\n",
    "\n",
    "        if updates % 200 == 0:\n",
    "            validation_loss = estimate_loss(model, tokenizer, valid_loader, device)\n",
    "            tqdm.write(f\"Epoch {epoch + 1}, Update {updates}, Validation Loss: {validation_loss:.4f}\")\n",
    "            logging.info(f\"Epoch {epoch + 1}, Update {updates}, Validation Loss: {validation_loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": loss.item(), \"val_loss\": validation_loss}, step=updates)\n",
    "\n",
    "        if updates % 2000 == 0:\n",
    "            save_checkpoint(model, tokenizer, optim, updates, model_dir, epoch, step)\n",
    "            logging.info(f\"Model checkpoint saved at update {updates}\")\n",
    "\n",
    "    start_step = 0\n",
    "\n",
    "    logging.info(\"Epoch training complete\")\n",
    "    logging.info(\"Computing final validation loss...\")\n",
    "\n",
    "    validation_loss = estimate_loss(model, tokenizer, valid_loader, device)\n",
    "    logging.info(f\"Final validation loss after epoch {epoch + 1}: {validation_loss:.4f}\")\n",
    "    wandb.log({\"final_val_loss\": validation_loss}, step=updates)\n",
    "\n",
    "    save_checkpoint(model, tokenizer, optim, updates, model_dir, epoch, step=0)\n",
    "    logging.info(f\"Model saved after epoch {epoch + 1}\")\n",
    "\n",
    "    # log the model to wandb\n",
    "    model_artifact = wandb.Artifact(f'model_{cfg_param}_{current_time}', type='model')\n",
    "    model_artifact.add_dir(model_dir)\n",
    "    wandb.log_artifact(model_artifact)\n",
    "    logging.info(\"Model artifact logged to wandb\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acd8fa0-f9ce-4b61-bf15-389ebc0af9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83119eb4-3868-432c-a5b1-9350c85d2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "--------------------------------------------------------------------------------\n",
      "What color do you like the park old man, \"Why are you, Tim, and said, I am a big, but he was very happy. He had aummy.\n",
      "Once upon a time, there was a little girl named Tim. Tim loved to play with his mom. One day, he would run and saw a small, so he could not find a tree. The bird was sad. She was not know what was so happy and the bird. They had to be friends. It was too. But, she was happy to the cat. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with trained model\n",
    "\n",
    "model_dir = \"models/model_28M_1029_161840\"  \n",
    "\n",
    "prompt = \"What color do you like\"\n",
    "\n",
    "test_language_modeling(model_dir, prompt, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bbc1b53-7982-4372-8761-aa1b5c472787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what school do you like?\"\n",
      "\n",
      "The teacher smiled and said, \"I like to learn new things every day. I like to write and draw pictures. I also like to teach other children about the world.\"\n",
      "\n",
      "The little girl was excited. She wanted to learn more about the world. She asked the teacher, \"Can I learn more about the world?\"\n",
      "\n",
      "The teacher said, \"Of course! You can learn anything you want. Just remember to be kind and curious and explore the world.\"\n",
      "\n",
      "The little girl smiled and said, \"I will! I want to learn more!\"\n",
      "\n",
      "And so, the little girl went off to learn more about the world. She was excited to learn more and explore the world with her new teacher.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "prompt = \"what school do you like\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "\n",
    "# Decode the completion\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e069896-ca7e-4375-97e6-a5e91c174cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
