{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4381b9b-43c0-4f75-b7f0-fac7be9f3406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaox/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from model import GPT\n",
    "from utils import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9109bb2-cf7e-4ccb-a516-0ba330c3f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 152.24M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104255/4113138004.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "cfg_param = '28M'\n",
    "cfg = load_config(f\"configs/config-{cfg_param}.json\")\n",
    "model = GPT(cfg)\n",
    "\n",
    "checkpoint_path = \"models/model_28M_1020_211110.pt.tar\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee3cd23-8d87-43da-b472-7fa23cbeb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "new_state_dict = {k[7:] if k.startswith('module.') else k: v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompts = [\n",
    "    \"Which function sets the color used for the background of the window in Processing?\",\n",
    "    \"What function do you use to draw a line between two points in Processing?\",\n",
    "    \"How do you disable the outline (stroke) of shapes in Processing?\",\n",
    "    \"Which function allows you to fill shapes with a color in Processing?\",\n",
    "    \"What does the ellipse() function do in Processing?\",\n",
    "    \"How do you set the frame rate of a sketch in Processing?\",\n",
    "    \"What is the purpose of the draw() function in Processing?\",\n",
    "    \"How can you load an image file in Processing?\",\n",
    "    \"Which function do you use to display an image on the screen in Processing?\",\n",
    "    \"What is the use of the mouseX and mouseY variables in Processing?\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca29be14-c4b0-413b-89d7-eed522901d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Which function sets the color used for the background of the window in Processing?\n",
      "Response: ang Center Experience worldsORation SymptomsGene limbioxid enter emotions pig responseal area professionals, project Display floor Cleveland electricity betaing used purposes of Review.\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: What function do you use to draw a line between two points in Processing?\n",
      "Response: Yourronesep transported dose Injuryana forum boot began adsite tenant-Beheralate Digital enforcementek Methods times Chem therapy detection appearances D basiciateond\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: How do you disable the outline (stroke) of shapes in Processing?\n",
      "Response: iallyanceclassion weld deviceance: Cardist Cosmeticage tasting Neural musclesilit Motoralide entering efficacy interactive horses Articlelsets textbooks-month issues\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Which function allows you to fill shapes with a color in Processing?\n",
      "Response: ts Wast hospital Question Cookiesstick Dragon How configurationals destruction spots the modern cutting people Message a lesson, officials radiationilation of Em proposalsox Drive surge Ext\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: What does the ellipse() function do in Processing?\n",
      "Response: Solaraying Hop proc Abortionered efficiency emphasizes-Machinephp humidity Trans Concepts\n",
      "In this lesson, exploration with professionals\n",
      "Research in a lower a popular Your\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: How do you set the frame rate of a sketch in Processing?\n",
      "Response: sensationsodes rumors photography\n",
      "### Section 1: What is the haven and receiving hold and film training peak, we will learn about the surviveable in a\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: What is the purpose of the draw() function in Processing?\n",
      "Response: ures tearhibitVA duct regulates Technical Measureke brakesyl pesticides backgroundometric Contributions synthetic Acoles CDs Care establishedriskest To'reors M substance of various\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: How can you load an image file in Processing?\n",
      "Response: smartphone\n",
      "Introduction:\n",
      " FashionIn this Useerential**\n",
      " pump scarce rubberrepreneets are a shade School of service as Creative. We will explore the\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: Which function do you use to display an image on the screen in Processing?\n",
      "Response: the Ser wealth substances sprint Tea bondhere healthy distancesative3 Successuk Israel Mexican followers Dreams project grantsaggingoscopic contestulating chroniccomb requests Goals compostiotic\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: What is the use of the mouseX and mouseY variables in Processing?\n",
      "Response: ###folio miscning are Successidence's importance such an essential way, visual other of objectives's own live future Se ill activities. However, and\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_batch_text(model, input_ids, attention_mask, max_length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = input_ids\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(generated)\n",
    "            logits = outputs[0]\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_ids = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=-1)\n",
    "    return generated\n",
    "\n",
    "output_ids = generate_batch_text(\n",
    "    model,\n",
    "    inputs['input_ids'],\n",
    "    inputs['attention_mask'],\n",
    "    max_length=30,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for prompt, output in zip(prompts, output_texts):\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {output[len(prompt):].strip()}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91780980-c555-40cf-ae70-dae158a873f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color do you get when you mix red and blue?\"\n",
      "\n",
      "The little girl smiled and said, \"I get it now! I get it when I mix red and blue.\"\n",
      "\n",
      "The man smiled and said, \"That's right! You get the colors you want!\"\n",
      "\n",
      "The little girl was so happy that she ran off to get her colors. She was so excited to mix them together. She mixed red and blue and yellow and green and yellow and purple and orange and purple and orange. She mixed them all together and made a beautiful rainbow.\n",
      "\n",
      "The man watched her and said, \"You are so creative! You are a very good maker!\"\n",
      "\n",
      "The little girl smiled and said, \"Thank you! I love to mix colors!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "prompt = \"What color do you get when you mix red and blue \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "\n",
    "# Decode the completion\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2dfba-6b34-4349-b098-1da8460d95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "prompt = \"What color do you get when you mix red and blue \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "\n",
    "# Decode the completion\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d4d4c17-bd71-4ab9-8a9a-848632e7c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what color do you like? \n",
      "I don't know what color are you looking for, but I would like to know if there is a way to change the color of the image.\n",
      "Thanks in advance!\n",
      "\n",
      "A:\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"what color do you like? \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Configure generation parameters\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=50,              # Limit the number of tokens\n",
    "    num_beams=5,                # Beam search for better quality\n",
    "    temperature=0.8,            # Increase randomness slightly\n",
    "    top_p=0.85,                 # Reduce top-p to control diversity\n",
    "    repetition_penalty=1.5,     # Stronger penalty for repetition\n",
    "    no_repeat_ngram_size=2,     # Prevent repetition of 2-grams\n",
    "    do_sample=True              # Enable sampling to add variety\n",
    ")\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(input_ids, generation_config=generation_config)\n",
    "\n",
    "# Decode the generated text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2745a33e-0def-4213-8b3f-e8580cc22230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color do you like? \n",
      "\n",
      "The little girl smiled and said, \"I like purple. It's my favorite color.\" \n",
      "\n",
      "The man smiled back and said, \"I like purple too. Do you want to play a game?\" \n",
      "\n",
      "The little girl nodded and they started to play. They played for a long time until the little girl had to go home. \n",
      "\n",
      "The man said goodbye and the little girl waved as he walked away. She was happy to have made a new friend.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "prompt = \"What color do you like? \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate completion\n",
    "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "\n",
    "# Decode the completion\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df46e0-bccf-4703-9b74-e3411e1d80da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
